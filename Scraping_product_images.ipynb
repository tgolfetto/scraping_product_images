{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sklqrOxc06tM15lLbjYRF9-PxTRQ8cq-",
      "authorship_tag": "ABX9TyPIOY4739ssrzFEfMwGXysj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Function to fetch a number of images (1 to N) for a specific product, using Google Images."
      ],
      "metadata": {
        "id": "TPRS1RgKJ2yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from random import randint\n",
        "from time import sleep\n",
        "\n",
        "# Headers to look less suspicios when scraping\n",
        "user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/602.2.14 (KHTML, like Gecko) Version/10.0.1 Safari/602.2.14'\n",
        "headers = {'User-Agent': user_agent,'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
        "\n",
        "# Fetch a number {limit} of images from Google Images querying for {product}\n",
        "def fetch_product(product, limit):\n",
        "  # Sleep between 1 to 5 seconds to change the request pattern\n",
        "  sleep(randint(1,5))\n",
        "  # Google Images URL\n",
        "  url = f\"https://www.google.com/search?q={product}&tbm=isch\"\n",
        "  response = requests.get(url)\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  #print(soup.prettify())\n",
        "  image_tags = soup.find_all('img', attrs={'class': 'DS1iW'}) # Class tag changes frequently\n",
        "  # Return the first {limit} images, if any\n",
        "  if image_tags:\n",
        "    result = []\n",
        "    for i in range(min(limit, len(image_tags))):\n",
        "      result.append(image_tags[i]['src'])\n",
        "    return result\n",
        "  else:\n",
        "    return [\"\"]"
      ],
      "metadata": {
        "id": "ooSicevlpxIC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to fill the data into the map that will generate the Dataframe"
      ],
      "metadata": {
        "id": "XtqS4HI3K96q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From a path, return the HTML tag of an image sourcing it\n",
        "def path_to_image_html(path):\n",
        "  print(path)\n",
        "  return '<img src=\"' + path + '\" width=\"100\" >'\n",
        "\n",
        "# Fill the various data in the map that will be used for the Dataframe\n",
        "def fill_data(data, row, picture):\n",
        "  data['Image'].append(picture)\n",
        "  data['Season'].append(row['Season'])\n",
        "  data['Brand'].append(row['Brand'])\n",
        "  data['SKU'].append(row['SKU'])\n",
        "  data['Description'].append(row['Description'])\n",
        "  data['Style'].append(row['Style'])\n",
        "  data['Colour Code'].append(row['Colour Code'])\n",
        "  data['Colour Desc'].append(row['Colour Desc'])\n",
        "  data['Size'].append(row['Size'])\n",
        "  data['Gender'].append(row['Gender'])\n",
        "  data['Category'].append(row['Category'])\n",
        "  data['Sub-Category'].append(row['Sub-Category'])\n",
        "  data['DDP-EUR'].append(row['DDP-EUR'])\n",
        "  data['Alpi Servizio Moda'].append(row['Alpi Servizio Moda'])\n",
        "  data['Alpi UK'].append(row['Alpi UK'])"
      ],
      "metadata": {
        "id": "d265htakEYJa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A CSV log is used to save the images that were scraped in the past."
      ],
      "metadata": {
        "id": "UUbwQqsDLjBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import csv\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the CSV log file on Google Drive\n",
        "log_file_path = '/content/drive/My Drive/Data/ATS_log.csv'\n",
        "\n",
        "log_map = {}\n",
        "\n",
        "# Fill the map containing SKU - Image path reading each line of the log\n",
        "with open(log_file_path, 'r') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "      if row['SKU'] not in log_map:\n",
        "        log_map[row['SKU']] = row['Image']"
      ],
      "metadata": {
        "id": "osgdj44Y_5UB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a708ff-7544-499c-9c95-eb3f42c6d2d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering products"
      ],
      "metadata": {
        "id": "6tbSITio-HlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypeVarTuple\n",
        "filters = {\n",
        "    'Season': [],\n",
        "    'Brand': ['13 09 SR'],\n",
        "    'Colour Code': [],\n",
        "    'Gender': ['WOMEN','UNISEX'],\n",
        "    'Category': [],\n",
        "    'Sub-Category': [],\n",
        "}\n",
        "\n",
        "whs_filters = {\n",
        "    'Alpi Servizio Moda': True,\n",
        "    'Alpi UK': True,\n",
        "    'CN_HK': False,\n",
        "    'DHL_KR': False,\n",
        "    '3RD Party Logistic': False\n",
        "}\n",
        "\n",
        "def match_filters(row):\n",
        "  for key, value in filters.items():\n",
        "    if len(value) > 0 and not row[key] in value:\n",
        "      print('Not matching filters')\n",
        "      return False\n",
        "  whs_match = False\n",
        "  for key, value in whs_filters.items():\n",
        "    if value == True and row[key] != '':\n",
        "      whs_match = True\n",
        "  if whs_match == False:\n",
        "      print('Not available in selected warehouse')\n",
        "    return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "J94xCqgY-GQP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the ATS file and fetch images for each product"
      ],
      "metadata": {
        "id": "y4NjGMEMd31D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "p5FvG9HAnBNA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "d06f54b4-3b46-48cc-f454-459a347abac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processing row: {'Internal ID': '1', 'Season': 'CARRYOVER', 'Brand': '13 09 SR', 'SKU': 'BINBLKSS22-BLACK-37', 'Description': 'Bingo pool slide', 'Style': 'BINBLKSS22', 'Colour Code': 'BLACK', 'Colour Desc': 'BLACK', 'Size': '37', 'Gender': 'WOMEN', 'Category': 'Shoes', 'Sub-Category': 'Flip Flops & Slides', 'DDP-EUR': '0', 'Alpi Servizio Moda': '', 'CN HK': '', 'DHL KR': '', '3RD Party Logistic': '', 'Alpi UK': '2'}\n",
            "Fetched https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTETScbsmF0DIX1IAJ6PhJrHHzCL15B0avesYqlktLDjjaXNXUGTnMvyY-DWtk&s\n",
            "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTETScbsmF0DIX1IAJ6PhJrHHzCL15B0avesYqlktLDjjaXNXUGTnMvyY-DWtk&s\n",
            "Processing row: {'Internal ID': '2', 'Season': 'CARRYOVER', 'Brand': '13 09 SR', 'SKU': 'BINBLKSS22-BLACK-38', 'Description': 'Bingo pool slide', 'Style': 'BINBLKSS22', 'Colour Code': 'BLACK', 'Colour Desc': 'BLACK', 'Size': '38', 'Gender': 'WOMEN', 'Category': 'Shoes', 'Sub-Category': 'Flip Flops & Slides', 'DDP-EUR': '0', 'Alpi Servizio Moda': '', 'CN HK': '', 'DHL KR': '', '3RD Party Logistic': '', 'Alpi UK': ''}\n",
            "Not matching WHS\n",
            "Processing row: {'Internal ID': '3', 'Season': 'CARRYOVER', 'Brand': '13 09 SR', 'SKU': 'BINBLKSS22-BLACK-42', 'Description': 'Bingo pool slide', 'Style': 'BINBLKSS22', 'Colour Code': 'BLACK', 'Colour Desc': 'BLACK', 'Size': '42', 'Gender': 'WOMEN', 'Category': 'Shoes', 'Sub-Category': 'Flip Flops & Slides', 'DDP-EUR': '0', 'Alpi Servizio Moda': '', 'CN HK': '', 'DHL KR': '', '3RD Party Logistic': '', 'Alpi UK': ''}\n",
            "Not matching WHS\n",
            "Processing row: {'Internal ID': '4', 'Season': 'CARRYOVER', 'Brand': '13 09 SR', 'SKU': 'COPBLKAW23-BLACK-38', 'Description': 'Copy', 'Style': 'COPBLKAW23', 'Colour Code': 'BLACK', 'Colour Desc': 'BLACK', 'Size': '38', 'Gender': 'WOMEN', 'Category': 'Shoes', 'Sub-Category': 'Boots', 'DDP-EUR': '0', 'Alpi Servizio Moda': '1', 'CN HK': '', 'DHL KR': '', '3RD Party Logistic': '', 'Alpi UK': ''}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-c40be24bb4ae>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0mfill_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpicture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlast_product_fetched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m           \u001b[0mpictures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fetched '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpictures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mlast_product_fetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-3e5d63e5dbd2>\u001b[0m in \u001b[0;36mfetch_product\u001b[0;34m(product, limit)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# Sleep between 1 to 5 seconds to change the request pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Google Images URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://www.google.com/search?q={product}&tbm=isch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import csv\n",
        "import pandas as pd\n",
        "from IPython.core.display import display,HTML\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the CSV file on Google Drive\n",
        "csv_file_path = '/content/drive/My Drive/Data/ATS_cut.csv'\n",
        "\n",
        "# Data to be print\n",
        "data = {\n",
        "    'Image': [],\n",
        "    'Season': [],\n",
        "    'Brand': [],\n",
        "    'SKU': [],\n",
        "    'Description': [],\n",
        "    'Style': [],\n",
        "    'Colour Code': [],\n",
        "    'Colour Desc': [],\n",
        "    'Size': [],\n",
        "    'Gender': [],\n",
        "    'Category': [],\n",
        "    'Sub-Category': [],\n",
        "    'DDP-EUR': [],\n",
        "    'Alpi Servizio Moda': [],\n",
        "    'Alpi UK': []\n",
        "}\n",
        "\n",
        "last_product_fetched = \"\"\n",
        "\n",
        "# Open the CSV file and read each line\n",
        "with open(csv_file_path, 'r') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "      # Only consider products in Italian warehouses\n",
        "      print('Processing row: ' + str(row))\n",
        "      if match_filters(row):\n",
        "        product = row['Brand'] + ' ' + row['Style'] # Text that will be queried\n",
        "        # Only fetch new products, skip duplicate queries and data saved on the log\n",
        "        if row['SKU'] in log_map:\n",
        "          picture = log_map[row['SKU']]\n",
        "          print('Reading from log ' + picture)\n",
        "          fill_data(data, row, picture)\n",
        "        elif product != last_product_fetched:\n",
        "          pictures = fetch_product(product, 1)\n",
        "          print('Fetched ' + pictures[0])\n",
        "          last_product_fetched = product\n",
        "          fill_data(data, row, path_to_image_html(pictures[0]))\n",
        "        else:\n",
        "          fill_data(data, row, path_to_image_html(pictures[0]))\n",
        "\n",
        "# Create a DataFrame from the data dictionary\n",
        "df = pd.DataFrame(data)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Save the DataFrame in HTML\n",
        "df.to_html('ATS.html', escape=False)\n",
        "\n",
        "# Save the DataFrame in CSV for logging purposes\n",
        "df.to_csv(log_file_path, index=False)\n"
      ]
    }
  ]
}